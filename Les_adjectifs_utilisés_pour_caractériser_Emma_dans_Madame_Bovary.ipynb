{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ee2401bcd24e4355b45fa2f015b8a2d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3a31169e85ea4e15a5c96e97f18ada25",
              "IPY_MODEL_a343b9748ba14d58b28215c00032336d",
              "IPY_MODEL_0b745b6b6d29414ebb5e53c314d38cdc"
            ],
            "layout": "IPY_MODEL_2a3329c14ce440efb3b0e3c5707dec40"
          }
        },
        "3a31169e85ea4e15a5c96e97f18ada25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_790922550f3849a69776b8b768617316",
            "placeholder": "​",
            "style": "IPY_MODEL_ca50fbfca9a84db384ff4d7373495fcb",
            "value": "NER chunks: 100%"
          }
        },
        "a343b9748ba14d58b28215c00032336d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40ce14944c644e33af6895fa95989f7e",
            "max": 383,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b397da8bcc434e1e83330cf41914f31f",
            "value": 383
          }
        },
        "0b745b6b6d29414ebb5e53c314d38cdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6958587a9af64d3fa16f08a370906238",
            "placeholder": "​",
            "style": "IPY_MODEL_8fa6c6edefe6456993337bd15f0b0297",
            "value": " 383/383 [00:17&lt;00:00, 18.32it/s]"
          }
        },
        "2a3329c14ce440efb3b0e3c5707dec40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "790922550f3849a69776b8b768617316": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca50fbfca9a84db384ff4d7373495fcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "40ce14944c644e33af6895fa95989f7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b397da8bcc434e1e83330cf41914f31f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6958587a9af64d3fa16f08a370906238": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fa6c6edefe6456993337bd15f0b0297": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "y6NEUo29teO9"
      },
      "outputs": [],
      "source": [
        "!wget https://www.gutenberg.org/cache/epub/14155/pg14155.txt -O Madame_Bovary.txt -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Méthode de découpage basée sur les normes des fichiers issus du Projet Gutenberg\n",
        "\n",
        "from pathlib import Path\n",
        "import re, os, tempfile, unicodedata, urllib.request\n",
        "\n",
        "DOWNLOAD_DIR = Path(\"/content/romans_Flaubert\")\n",
        "DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "FLAUBERT = {\n",
        "    \"Madame_Bovary\": \"https://www.gutenberg.org/cache/epub/14155/pg14155.txt\",\n",
        "}\n",
        "\n",
        "START_PATTERNS = [\n",
        "    re.compile(r'(?i)(?:\\*+\\s*)?(start|begin)(?:\\s+of)?(?:\\s+the)?\\s+project\\s+gutenberg'),\n",
        "    re.compile(r'(?i)start\\s+of\\s+this\\s+project\\s+gutenberg'),\n",
        "]\n",
        "END_PATTERNS = [\n",
        "    re.compile(r'(?i)(?:\\*+\\s*)?(end|finish|finis?h|stop)(?:\\s+of)?(?:\\s+the)?\\s+project\\s+gutenberg'),\n",
        "    re.compile(r'(?i)end\\s+of\\s+this\\s+project\\s+gutenberg'),\n",
        "]\n",
        "\n",
        "def normalize(s): return unicodedata.normalize(\"NFKC\", s).replace(\"\\u00A0\", \" \")\n",
        "\n",
        "def find_marker(lines, patterns, rev=False):\n",
        "    rng = range(len(lines)-1, -1, -1) if rev else range(len(lines))\n",
        "    for i in rng:\n",
        "        if any(p.search(normalize(lines[i]).strip()) for p in patterns):\n",
        "            return i\n",
        "    return None\n",
        "\n",
        "def gutenberg_strip_text(text):\n",
        "    text = normalize(text).replace('\\r\\n', '\\n')\n",
        "    lines = text.splitlines(keepends=True)\n",
        "    s = find_marker(lines, START_PATTERNS)\n",
        "    e = find_marker(lines, END_PATTERNS, rev=True)\n",
        "    if s is None and e is None: return text\n",
        "    start = (s + 1) if s is not None else 0\n",
        "    end = e if e is not None else len(lines)\n",
        "    if start >= end: return text\n",
        "    body = lines[start:end]\n",
        "    while body and not body[0].strip(): body.pop(0)\n",
        "    while body and not body[-1].strip(): body.pop()\n",
        "    return ''.join(body)\n",
        "\n",
        "def download_if_missing(url, dest):\n",
        "    if dest.exists(): return\n",
        "    try:\n",
        "        urllib.request.urlretrieve(url, dest)\n",
        "    except Exception:\n",
        "        pass  # pas d'affichage d'erreur\n",
        "\n",
        "def safe_write(path, content):\n",
        "    with tempfile.NamedTemporaryFile(\"w\", delete=False, encoding=\"utf-8\", dir=path.parent) as tmp:\n",
        "        tmp.write(content)\n",
        "    os.replace(tmp.name, path)\n",
        "\n",
        "def process(folder):\n",
        "    for p in sorted(folder.glob(\"*.txt\")):\n",
        "        txt = p.read_text(encoding=\"utf-8\", errors=\"replace\")\n",
        "        stripped = gutenberg_strip_text(txt)\n",
        "        if stripped != txt:\n",
        "            safe_write(p, stripped)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    for title, url in FLAUBERT.items():\n",
        "        download_if_missing(url, DOWNLOAD_DIR / f\"{title}.txt\")\n",
        "    process(DOWNLOAD_DIR)\n",
        "    print(\"Découpage terminé. Voir le dossier /content/romans_Flaubert\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "xUsqvzJjyUCE",
        "outputId": "1fd43b1f-71d0-49c5-f1b9-ba16db35f1c8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Découpage terminé. Voir le dossier /content/romans_Flaubert\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Extraction de toutes les entités nommées de Madame Bovary avec Camembert NER\n",
        "\n",
        "!pip install -q transformers[torch] datasets sentencepiece nltk\n",
        "!pip install -q --upgrade accelerate\n",
        "\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import json, csv, os\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "MODEL_NAME = \"Jean-Baptiste/camembert-ner\"\n",
        "TXT_PATH = \"/content/Madame_Bovary.txt\"\n",
        "OUTPUT_DIR = \"/content/ner_bovary\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "DEVICE = 0\n",
        "\n",
        "# Load model and pipeline\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "model = AutoModelForTokenClassification.from_pretrained(MODEL_NAME)\n",
        "try:\n",
        "    nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, device=DEVICE, aggregation_strategy=\"simple\")\n",
        "except TypeError:\n",
        "    nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, device=DEVICE, grouped_entities=True)\n",
        "\n",
        "# Read text and split into sentences\n",
        "with open(TXT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "sentences = sent_tokenize(text, language='french')\n",
        "print(f\"Total sentences: {len(sentences)}\")\n",
        "\n",
        "# Build sentence spans for mapping\n",
        "sentence_spans = []\n",
        "cursor = 0\n",
        "for sent in sentences:\n",
        "    start = text.find(sent, cursor)\n",
        "    if start == -1:\n",
        "        start = cursor\n",
        "    end = start + len(sent)\n",
        "    sentence_spans.append((sent, start, end))\n",
        "    cursor = end\n",
        "\n",
        "# Build chunks for processing\n",
        "max_tokens = tokenizer.model_max_length\n",
        "MARGIN = 32\n",
        "chunks = []\n",
        "chunk_starts = []\n",
        "current_chunk = \"\"\n",
        "current_start = None\n",
        "\n",
        "for sent, s_start, s_end in sentence_spans:\n",
        "    candidate = f\"{current_chunk} {sent}\".strip() if current_chunk else sent\n",
        "    token_len = len(tokenizer(candidate, return_tensors=\"pt\")[\"input_ids\"][0])\n",
        "\n",
        "    if token_len + MARGIN < max_tokens:\n",
        "        if not current_chunk:\n",
        "            current_start = s_start\n",
        "        current_chunk = candidate\n",
        "    else:\n",
        "        if current_chunk:\n",
        "            chunks.append(current_chunk)\n",
        "            chunk_starts.append(current_start)\n",
        "        current_chunk = sent\n",
        "        current_start = s_start\n",
        "\n",
        "if current_chunk:\n",
        "    chunks.append(current_chunk)\n",
        "    chunk_starts.append(current_start)\n",
        "\n",
        "print(f\"Total chunks: {len(chunks)}\")\n",
        "\n",
        "# Find sentence containing entity\n",
        "def find_sentence(abs_start, abs_end):\n",
        "    for s_text, s_start, s_end in sentence_spans:\n",
        "        if s_start <= abs_start and abs_end <= s_end:\n",
        "            return s_text.strip()\n",
        "    return text[max(0, abs_start-200):min(len(text), abs_end+200)]\n",
        "\n",
        "# Run NER and collect entities\n",
        "all_entities = []\n",
        "for chunk_text, chunk_start in tqdm(list(zip(chunks, chunk_starts)), desc=\"NER chunks\"):\n",
        "    results = nlp(chunk_text)\n",
        "    for ent in results:\n",
        "        start = ent.get(\"start\")\n",
        "        end = ent.get(\"end\")\n",
        "        if start is None or end is None:\n",
        "            continue\n",
        "\n",
        "        abs_start = chunk_start + start\n",
        "        abs_end = chunk_start + end\n",
        "        ent_type = ent.get(\"entity_group\") or ent.get(\"entity\") or ent.get(\"label\")\n",
        "        ent_text = ent.get(\"word\", text[abs_start:abs_end])\n",
        "        sentence = find_sentence(abs_start, abs_end)\n",
        "\n",
        "        all_entities.append({\n",
        "            \"text\": ent_text,\n",
        "            \"type\": ent_type,\n",
        "            \"sentence\": sentence,\n",
        "            \"key\": (abs_start, abs_end, ent_type, ent_text)\n",
        "        })\n",
        "\n",
        "print(f\"Raw entities: {len(all_entities)}\")\n",
        "\n",
        "# Deduplicate by keeping first occurrence of each unique entity\n",
        "seen = set()\n",
        "entities_dedup = []\n",
        "for e in all_entities:\n",
        "    if e[\"key\"] not in seen:\n",
        "        seen.add(e[\"key\"])\n",
        "        entities_dedup.append({\"text\": e[\"text\"], \"type\": e[\"type\"], \"sentence\": e[\"sentence\"]})\n",
        "\n",
        "print(f\"Entities after dedup: {len(entities_dedup)}\")\n",
        "\n",
        "# Save entities\n",
        "json_path = os.path.join(OUTPUT_DIR, \"madame_bovary_ner.json\")\n",
        "csv_path = os.path.join(OUTPUT_DIR, \"madame_bovary_ner.csv\")\n",
        "\n",
        "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(entities_dedup, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "with open(csv_path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=[\"text\", \"type\", \"sentence\"])\n",
        "    writer.writeheader()\n",
        "    writer.writerows(entities_dedup)\n",
        "\n",
        "print(f\"Saved {len(entities_dedup)} entities to:\")\n",
        "print(f\"  - {json_path}\")\n",
        "print(f\"  - {csv_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263,
          "referenced_widgets": [
            "ee2401bcd24e4355b45fa2f015b8a2d1",
            "3a31169e85ea4e15a5c96e97f18ada25",
            "a343b9748ba14d58b28215c00032336d",
            "0b745b6b6d29414ebb5e53c314d38cdc",
            "2a3329c14ce440efb3b0e3c5707dec40",
            "790922550f3849a69776b8b768617316",
            "ca50fbfca9a84db384ff4d7373495fcb",
            "40ce14944c644e33af6895fa95989f7e",
            "b397da8bcc434e1e83330cf41914f31f",
            "6958587a9af64d3fa16f08a370906238",
            "8fa6c6edefe6456993337bd15f0b0297"
          ]
        },
        "cellView": "form",
        "id": "XZvTJnd2toI2",
        "outputId": "c2d145bb-6188-4f5f-e4f9-447e7f258526"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Device set to use cuda:0\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (515 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sentences: 6801\n",
            "Total chunks: 383\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "NER chunks:   0%|          | 0/383 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee2401bcd24e4355b45fa2f015b8a2d1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw entities: 3109\n",
            "Entities after dedup: 3109\n",
            "Saved 3109 entities to:\n",
            "  - /content/ner_bovary/madame_bovary_ner.json\n",
            "  - /content/ner_bovary/madame_bovary_ner.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from collections import Counter\n",
        "\n",
        "# Charger les résultats de l'analyse NER\n",
        "with open(\"/content/ner_bovary/madame_bovary_ner.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    entities = json.load(f)\n",
        "per_entities = [e[\"text\"] for e in entities if e[\"type\"] == \"PER\"]\n",
        "\n",
        "entity_counts = Counter(per_entities)\n",
        "\n",
        "# Trier par ordre décroissant de fréquence\n",
        "sorted_entities = entity_counts.most_common()\n",
        "\n",
        "print(sorted_entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfFFdW3Etus1",
        "outputId": "af809300-84c9-4693-962e-e91a2ac08fcf"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Charles', 302), ('Emma', 199), ('Léon', 115), ('Homais', 103), ('Rodolphe', 95), ('Madame', 43), ('Lheureux', 42), ('M. Homais', 38), ('Justin', 36), ('Binet', 28), ('Félicité', 26), ('M. Lheureux', 24), ('Hippolyte', 24), ('Monsieur', 21), ('madame Homais', 18), ('Rouault', 16), ('Hivert', 15), ('Berthe', 15), ('Lefrançois', 14), ('madame Lefrançois', 14), ('père Rouault', 13), ('Tuvache', 13), ('Canivet', 13), ('or', 11), ('Rodol', 11), ('Vinçart', 11), ('Lestiboudois', 10), ('Conseiller', 10), ('Lion d', 9), ('Rolet', 9), ('M. Guillaumin', 9), ('M. Bournisien', 9), ('M. Canivet', 9), ('Bournisien', 9), ('Vicomte', 8), ('M. Léon', 8), ('Tostes', 7), ('M. Binet', 7), ('Voltaire', 7), ('Napoléon', 7), ('Athalie', 7), ('M. Bovary', 7), ('M. Boulanger', 7), ('madame Tuvache', 7), ('Bo', 7), ('Bridoux', 7), ('Aveugle', 7), ('Bertaux', 6), ('M.', 6), ('Artémise', 6), ('M. Tuvache', 6), ('Bovary', 6), ('Nastasie', 5), ('l', 5), ('Hirondelle', 5), ('Dieu', 5), ('madame', 5), ('L', 5), ('Théodore', 5), ('ol', 5), ('M. Derozerays', 5), ('Lagardy', 5), ('M. Rouault', 4), ('.', 4), ('Marquis', 4), ('Vaubyessard', 4), ('la Vierge', 4), ('F', 4), ('Yonville', 4), ('Monseigneur', 4), ('Madame Homais', 4), ('M. Lieuvain', 4), ('Langlois', 4), ('madame Caron', 4), ('Héloïse', 3), ('Walter Scott', 3), ('M. et madame Bovary', 3), ('Marquise', 3), ('Tellier', 3), ('Franklin', 3), ('. Léon', 3), ('abbé Bournisien', 3), ('Riboudet', 3), ('mère Lefrançois', 3), ('d', 3), ('M. Rodolphe', 3), ('Girard', 3), ('Larivière', 3), ('Edgar', 3), ('Ashton', 3), ('Lucie', 3), ('Lempereur', 3), ('Barneville', 3), ('M. Larivière', 3), ('Proviseur', 2), ('M', 2), ('Béranger', 2), ('madame Dubuc', 2), ('mademoiselle Emma', 2), ('Mademoiselle Rouault', 2), ('mademoiselle Rouault', 2), ('Saint-Barthélemy', 2), ('ma', 2), ('Djali', 2), ('Andervilliers', 2), ('Le Marquis', 2), ('Pompadour', 2), ('veuve Lefrançois', 2), ('monsieur Homais', 2), ('Guillaumin', 2), ('Rousseau', 2), ('madame Bo', 2), ('tz', 2), ('père Tellier', 2), ('Vierge', 2), ('Boudet', 2), ('madame Bovary', 2), ('M. le Conseiller', 2), ('Catherine-Nicaise-Élisabeth Leroux', 2), ('Irma', 2), ('Hip', 2), ('Fanal', 2), ('Rouen', 2), ('. Bournisien', 2), ('Curé', 2), ('Liégeard', 2), ('Dubreuil', 2), ('M. Vinçart', 2), ('Hareng', 2), ('Dubocage', 2), ('Gustave Flaubert', 1), ('Marie-Antoine-Jules Senard', 1), ('GUSTAVE FLAUBERT', 1), ('Louis Bouilhet', 1), ('Roger', 1), ('. Charles-Denis-Bartholomé Bovary', 1), ('Saint-Romain', 1), ('Minerve', 1), ('Mademoiselle Emma', 1), ('Dubuc', 1), ('bon Dieu', 1), ('Maître Rouault', 1), ('Père Rouault', 1), ('madame Charles', 1), ('Hippocrate', 1), ('Domingo', 1), ('Fidèle', 1), ('mademoiselle de la Vallière', 1), ('Cour', 1), ('M. le vicaire', 1), ('Sacré-Cœur', 1), ('Jésus', 1), ('abbé Frayssinous', 1), ('Marie Stuart', 1), ('Jeanne d', 1), ('Arc', 1), ('Agnès Sorel', 1), ('Ferronnière', 1), ('Clémence Isaure', 1), ('saint Louis', 1), ('Bayard', 1), ('Louis XI', 1), ('Béarn', 1), ('Louis XIV', 1), ('Éternel', 1), ('mon Dieu', 1), ('marquis d', 1), ('M. le Marquis', 1), ('Jean-Antoine d', 1), ('Andervilliers d’Yverbonville', 1), ('comte de la Vaubye', 1), ('la Fres', 1), ('Jean-Antoine-Henry-Guy d’Andervilliers de la Vaubyessard', 1), ('vieux duc de Laverdière', 1), ('comte d', 1), ('Artois', 1), ('marquis de Conflans', 1), ('Marie-Antoinette', 1), ('. de Coigny', 1), ('de Lauzun', 1), ('Miss Arabelle', 1), ('Romulus', 1), ('Trafalgar', 1), ('mademoiselle d’Andervilliers', 1), ('mademoiselle d', 1), ('époux Bovary', 1), ('Bois', 1), ('Opéra', 1), ('Eugène Sue', 1), ('Balzac', 1), ('George Sand', 1), ('Poussière d', 1), ('marquis d’Andervilliers', 1), ('Érard', 1), ('Amour', 1), ('Charles X', 1), ('M. Homais!', 1), ('Raspail', 1), ('Darcet', 1), ('Regnault', 1), ('Polyte', 1), ('Être suprême', 1), ('Créateur', 1), ('Mon', 1), ('Socrate', 1), ('II Emma', 1), ('M. Léon Dupuis', 1), ('monsieur Bovary', 1), ('M. votre', 1), ('Yanoda', 1), ('Delille', 1), ('cité', 1), ('Madame Lefrançois', 1), ('M. le curé', 1), ('stes', 1), ('Georges', 1), ('Madeleine', 1), ('. Homais', 1), ('Racine', 1), ('Mathieu Laensberg', 1), ('Renommée', 1), ('Camus', 1), ('madame Bovar', 1), ('M. Bo', 1), ('M Homais', 1), ('Routot', 1), ('Trois Frères', 1), ('Barbe d', 1), ('Grand Sauvage', 1), ('Yonvillais', 1), ('Sachette', 1), ('Guérine', 1), ('Guérin', 1), ('saint Paul', 1), ('Longuermarre', 1), ('divin Fils', 1), ('Rodolphe Boulanger de la Huchette', 1), ('. Boulanger', 1), ('Rodolphe Boulanger', 1), ('Virginie', 1), ('M. le', 1), ('M. Derozerays de la Panville', 1), ('Boulanger', 1), ('Lest', 1), ('ois', 1), ('Lieuvain', 1), ('. Derozerays', 1), ('M. le président', 1), ('Cincinnatus', 1), ('Dioclétien', 1), ('Bizet', 1), ('Quincampoix', 1), ('M. Caron', 1), ('Argueil', 1), ('M. Bain', 1), ('de Givry-Saint-Martin', 1), ('M. Belot', 1), ('de Notre-Dame', 1), ('Lehérissé', 1), ('Cullembourg', 1), ('Catherine Leroux', 1), ('eur Tuvache', 1), ('M. Liégeard', 1), ('M. Leplichey', 1), ('de Loyola', 1), ('Rod', 1), ('M. Alexandre', 1), ('capitaine Binet', 1), ('THEODORE ROUAULT', 1), ('docteur Duval', 1), ('Achille', 1), ('Ambroise Paré', 1), ('Celse', 1), ('Dupuytren', 1), ('Gensoul', 1), ('Hippolyte Tautain', 1), ('te', 1), ('Marie', 1), ('Notre Père', 1), ('. Canivet', 1), ('Saint-Pierre', 1), ('duc de Clarence', 1), ('Em', 1), ('Sauveur', 1), ('Dieu le Père', 1), ('Le Curé', 1), ('M. Boulard', 1), ('M. de Maistre', 1), ('M. de***', 1), ('Seigneur', 1), ('la Vallière', 1), ('Christ', 1), ('madame Langlois', 1), ('madame Dubreuil', 1), ('monsieur Bournisien', 1), ('Galilée', 1), ('Charles immédiatement', 1), ('Lucie de Lamermoor', 1), ('Edgar-Lagardy', 1), ('Gilbert', 1), ('Arthur', 1), ('Tamburini', 1), ('Rubini', 1), ('Persiani', 1), ('Grisi', 1), ('Muse', 1), ('clématite', 1), ('Tour de Nesle', 1), ('. Lormeaux', 1), ('Marianne', 1), ('Pierre de Brézé', 1), ('Louis de Brézé', 1), ('Diane de Poitiers', 1), ('comtesse de Brézé', 1), ('duchesse de', 1), ('sainte Vierge', 1), ('Louis XII', 1), ('Richard Cœur de Lion', 1), ('roi d', 1), ('Angleterre', 1), ('Roi', 1), ('Pierre Corneille', 1), ('épée', 1), ('Damoclès', 1), ('Adolphe', 1), ('Dodolphe', 1), ('IV Léon', 1), ('Cupidon', 1), ('LEMPEREUR', 1), ('M. Lormeaux', 1), ('Lormeaux', 1), ('mademoiselle Lempereur', 1), ('Louis XIII', 1), ('Pomard', 1), ('Cujas', 1), ('Bartole', 1), ('Thomassin', 1), ('Serviteur', 1), ('Qu', 1), ('Annette!', 1), ('Maître Hareng', 1), ('maître Hareng', 1), ('Morel', 1), ('Steuben', 1), ('Schopin', 1), ('Providence', 1), ('Mère Rolet', 1), ('mère Rolet', 1), ('Boulle', 1), ('docteur Larivière', 1), ('Bichat', 1), ('Docteur', 1), ('Cadet de Gassicourt', 1), ('sieur Tuvache', 1), ('Homme-Dieu', 1), ('Jésus-Christ', 1), ('d’Holbach', 1), ('Nicolas', 1), ('éli', 1), ('la sainte Vierge', 1), ('estiboudois', 1), ('Mademoiselle Lempereur', 1), ('veuve Dupuis', 1), ('. Léon Dupuis', 1), ('Léocadie Lebœuf', 1), ('Moyen Age', 1), ('Pulvermacher', 1), ('Scythe', 1), ('Vesta', 1), ('Vaufrylard', 1), ('Pythagore', 1), ('M. le préfet', 1), ('Henri IV', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Toutes les occurences de l'entité nommée \"Emma\" : répartitions des types (PER, LOC, MISC, etc.)\n",
        "\n",
        "import json\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# Input paths\n",
        "INPUT_DIR = \"/content/ner_bovary\"\n",
        "JSON_PATH = os.path.join(INPUT_DIR, \"madame_bovary_ner.json\")\n",
        "CSV_PATH = os.path.join(INPUT_DIR, \"madame_bovary_ner.csv\")\n",
        "\n",
        "# Output paths\n",
        "OUTPUT_DIR = \"/content/ner_bovary\"\n",
        "EMMA_PER_JSON = os.path.join(OUTPUT_DIR, \"emma_per_only.json\")\n",
        "EMMA_PER_CSV = os.path.join(OUTPUT_DIR, \"emma_per_only.csv\")\n",
        "EMMA_ALL_JSON = os.path.join(OUTPUT_DIR, \"emma_all_types.json\")\n",
        "EMMA_ALL_CSV = os.path.join(OUTPUT_DIR, \"emma_all_types.csv\")\n",
        "\n",
        "# Load the data (try JSON first, fall back to CSV)\n",
        "entities = []\n",
        "try:\n",
        "    with open(JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "        entities = json.load(f)\n",
        "    print(f\"Loaded {len(entities)} entities from JSON\")\n",
        "except FileNotFoundError:\n",
        "    print(\"JSON file not found, trying CSV...\")\n",
        "    with open(CSV_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        entities = list(reader)\n",
        "    print(f\"Loaded {len(entities)} entities from CSV\")\n",
        "\n",
        "# Filter 1: Only sentences containing \"Emma\" AND type is \"PER\"\n",
        "emma_per_entities = [\n",
        "    e for e in entities\n",
        "    if \"Emma\" in e.get(\"sentence\", \"\") and e.get(\"type\") == \"PER\"\n",
        "]\n",
        "\n",
        "# Filter 2: All sentences containing \"Emma\" (any type)\n",
        "emma_all_entities = [\n",
        "    e for e in entities\n",
        "    if \"Emma\" in e.get(\"sentence\", \"\")\n",
        "]\n",
        "\n",
        "print(f\"\\nFilter results:\")\n",
        "print(f\"  - Emma with type PER: {len(emma_per_entities)} entities\")\n",
        "print(f\"  - Emma all types: {len(emma_all_entities)} entities\")\n",
        "\n",
        "# Check for non-PER types\n",
        "non_per_types = set(e.get(\"type\") for e in emma_all_entities if e.get(\"type\") != \"PER\")\n",
        "if non_per_types:\n",
        "    print(f\"  - Non-PER types found: {', '.join(sorted(non_per_types))}\")\n",
        "\n",
        "# Save Filter 1: Emma PER only\n",
        "with open(EMMA_PER_JSON, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(emma_per_entities, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "with open(EMMA_PER_CSV, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "    if emma_per_entities:\n",
        "        writer = csv.DictWriter(f, fieldnames=[\"text\", \"type\", \"sentence\"])\n",
        "        writer.writeheader()\n",
        "        writer.writerows(emma_per_entities)\n",
        "\n",
        "# Save Filter 2: Emma all types\n",
        "with open(EMMA_ALL_JSON, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(emma_all_entities, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "with open(EMMA_ALL_CSV, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "    if emma_all_entities:\n",
        "        writer = csv.DictWriter(f, fieldnames=[\"text\", \"type\", \"sentence\"])\n",
        "        writer.writeheader()\n",
        "        writer.writerows(emma_all_entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "3KROhXQetxlG",
        "outputId": "53089816-66b0-4d98-8ea7-8311006638de"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 3109 entities from JSON\n",
            "\n",
            "Filter results:\n",
            "  - Emma with type PER: 331 entities\n",
            "  - Emma all types: 562 entities\n",
            "  - Non-PER types found: LOC, MISC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install stanza -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E19HF2Xfu1nN",
        "outputId": "9a41aca9-2260-4a8e-a1a2-5cfa5abbf663"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m1.4/1.7 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/608.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import stanza\n",
        "import json\n",
        "import csv\n",
        "import os\n",
        "from collections import defaultdict\n",
        "\n",
        "# Setup Stanza\n",
        "print(\"Setting up Stanza for French...\")\n",
        "stanza.download('fr', verbose=False)\n",
        "nlp = stanza.Pipeline('fr', verbose=False)\n",
        "\n",
        "# Input paths\n",
        "INPUT_DIR = \"/content/ner_bovary\"\n",
        "EMMA_ALL_JSON = os.path.join(INPUT_DIR, \"emma_all_types.json\")\n",
        "EMMA_ALL_CSV = os.path.join(INPUT_DIR, \"emma_all_types.csv\")\n",
        "\n",
        "# Load Emma sentences (try JSON first, fall back to CSV)\n",
        "emma_entities = []\n",
        "try:\n",
        "    with open(EMMA_ALL_JSON, \"r\", encoding=\"utf-8\") as f:\n",
        "        emma_entities = json.load(f)\n",
        "    print(f\"Loaded {len(emma_entities)} Emma entities from JSON\")\n",
        "except FileNotFoundError:\n",
        "    print(\"JSON file not found, trying CSV...\")\n",
        "    with open(EMMA_ALL_CSV, \"r\", encoding=\"utf-8\") as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        emma_entities = list(reader)\n",
        "    print(f\"Loaded {len(emma_entities)} Emma entities from CSV\")\n",
        "\n",
        "# Extract unique sentences containing Emma\n",
        "emma_sentences = list(set(e.get(\"sentence\", \"\") for e in emma_entities if e.get(\"sentence\")))\n",
        "print(f\"Found {len(emma_sentences)} unique sentences containing Emma\")\n",
        "\n",
        "# Process sentences with Stanza and extract adjectives\n",
        "print(\"\\nProcessing sentences with Stanza to extract adjectives...\")\n",
        "sentence_adjectives = []\n",
        "\n",
        "for i, sentence in enumerate(emma_sentences, 1):\n",
        "    if i % 50 == 0:\n",
        "        print(f\"  Processing sentence {i}/{len(emma_sentences)}...\")\n",
        "\n",
        "    doc = nlp(sentence)\n",
        "\n",
        "    # Extract adjectives (POS tag = ADJ)\n",
        "    adjectives = []\n",
        "    for sent in doc.sentences:\n",
        "        for word in sent.words:\n",
        "            if word.upos == \"ADJ\":\n",
        "                adjectives.append(word.text)\n",
        "\n",
        "    if adjectives:\n",
        "        sentence_adjectives.append({\n",
        "            \"sentence\": sentence,\n",
        "            \"adjectives\": adjectives,\n",
        "            \"adjective_count\": len(adjectives)\n",
        "        })\n",
        "\n",
        "print(f\"\\nResults:\")\n",
        "print(f\"  - Sentences with adjectives: {len(sentence_adjectives)}\")\n",
        "print(f\"  - Total adjectives found: {sum(s['adjective_count'] for s in sentence_adjectives)}\")\n",
        "\n",
        "# Display sample results\n",
        "print(\"\\nSample sentences with adjectives:\")\n",
        "for i, item in enumerate(sentence_adjectives[:5], 1):\n",
        "    print(f\"\\n{i}. Adjectives: {', '.join(item['adjectives'])}\")\n",
        "    print(f\"   Sentence: {item['sentence'][:100]}{'...' if len(item['sentence']) > 100 else ''}\")\n",
        "\n",
        "# Save results\n",
        "OUTPUT_JSON = os.path.join(INPUT_DIR, \"emma_sentences_adjectives.json\")\n",
        "OUTPUT_CSV = os.path.join(INPUT_DIR, \"emma_sentences_adjectives.csv\")\n",
        "\n",
        "with open(OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(sentence_adjectives, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# For CSV, join adjectives as comma-separated string\n",
        "with open(OUTPUT_CSV, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=[\"sentence\", \"adjectives\", \"adjective_count\"])\n",
        "    writer.writeheader()\n",
        "    for item in sentence_adjectives:\n",
        "        writer.writerow({\n",
        "            \"sentence\": item[\"sentence\"],\n",
        "            \"adjectives\": \", \".join(item[\"adjectives\"]),\n",
        "            \"adjective_count\": item[\"adjective_count\"]\n",
        "        })\n",
        "\n",
        "print(f\"\\nSaved results to:\")\n",
        "print(f\"  - {OUTPUT_JSON}\")\n",
        "print(f\"  - {OUTPUT_CSV}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "ohG3Q5WFuC2v",
        "outputId": "e35589cd-4a4c-4ccd-9ad5-558bdc677498"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up Stanza for French...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-241358493.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Setting up Stanza for French...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mstanza\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstanza\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Input paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/stanza/pipeline/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang, dir, package, processors, logging_level, verbose, use_gpu, model_dir, download_method, resources_url, resources_branch, resources_version, resources_filepath, proxies, foundation_cache, device, allow_unknown_language, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m                 \u001b[0;31m# try to build processor, throw an exception if there is a requirements issue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m                 self.processors[processor_name] = NAME_TO_PROCESSOR_CLASS[processor_name](config=curr_processor_config,\n\u001b[0m\u001b[1;32m    309\u001b[0m                                                                                           \u001b[0mpipeline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                                                                                           device=self.device)\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/stanza/pipeline/processor.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, pipeline, device)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_variant'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_up_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;31m# build the final config for the processor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/stanza/pipeline/pos_processor.py\u001b[0m in \u001b[0;36m_set_up_model\u001b[0;34m(self, config, pipeline, device)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 'charlm_backward_file': config.get('backward_charlm_path', None)}\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# set up trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfoundation_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfoundation_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tqdm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'tqdm'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tqdm'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/stanza/models/pos/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, vocab, pretrain, model_file, device, foundation_cache)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;31m# load everything from file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfoundation_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfoundation_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;31m# build model from scratch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/stanza/models/pos/trainer.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, filename, pretrain, args, foundation_cache)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0memb_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pretrain'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpretrain\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# we use pretrain only if args['pretrain'] == True and pretrain is not None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0memb_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert_model.\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model %s has a finetuned transformer.  Not using transformer cache to make sure the finetuned version of the transformer isn't accidentally used elsewhere\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/stanza/models/common/pretrain.py\u001b[0m in \u001b[0;36memb\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_emb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_emb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/stanza/models/common/pretrain.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0;31m# TODO: after making the next release, remove the weights_only=False version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mUnpicklingError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1545\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1546\u001b[0;31m                 return _legacy_load(\n\u001b[0m\u001b[1;32m   1547\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1548\u001b[0m                     \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1810\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1811\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1812\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1814\u001b[0m     \u001b[0mdeserialized_storage_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_weights_only_unpickler.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\">d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mBINUNICODE\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m                 \u001b[0mstrlen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<I\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mstrlen\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"String is too long\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m spacy download fr_dep_news_trf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSV03kW4u7DV",
        "outputId": "b5714243-bb8b-4628-cb5c-397d7b64a361"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fr-dep-news-trf==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_dep_news_trf-3.8.0/fr_dep_news_trf-3.8.0-py3-none-any.whl (397.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.7/397.7 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting spacy-curated-transformers<1.0.0,>=0.2.2 (from fr-dep-news-trf==3.8.0)\n",
            "  Downloading spacy_curated_transformers-0.3.1-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.12/dist-packages (from fr-dep-news-trf==3.8.0) (0.2.1)\n",
            "Collecting protobuf<3.21.0 (from fr-dep-news-trf==3.8.0)\n",
            "  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Collecting curated-transformers<0.2.0,>=0.1.0 (from spacy-curated-transformers<1.0.0,>=0.2.2->fr-dep-news-trf==3.8.0)\n",
            "  Downloading curated_transformers-0.1.1-py2.py3-none-any.whl.metadata (965 bytes)\n",
            "Collecting curated-tokenizers<0.1.0,>=0.0.9 (from spacy-curated-transformers<1.0.0,>=0.2.2->fr-dep-news-trf==3.8.0)\n",
            "  Downloading curated_tokenizers-0.0.9-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from spacy-curated-transformers<1.0.0,>=0.2.2->fr-dep-news-trf==3.8.0) (2.8.0+cu126)\n",
            "Requirement already satisfied: regex>=2022 in /usr/local/lib/python3.12/dist-packages (from curated-tokenizers<0.1.0,>=0.0.9->spacy-curated-transformers<1.0.0,>=0.2.2->fr-dep-news-trf==3.8.0) (2024.11.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->fr-dep-news-trf==3.8.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->fr-dep-news-trf==3.8.0) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->fr-dep-news-trf==3.8.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->fr-dep-news-trf==3.8.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->fr-dep-news-trf==3.8.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->fr-dep-news-trf==3.8.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->fr-dep-news-trf==3.8.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->fr-dep-news-trf==3.8.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->fr-dep-news-trf==3.8.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->fr-dep-news-trf==3.8.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->fr-dep-news-trf==3.8.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->fr-dep-news-trf==3.8.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->fr-dep-news-trf==3.8.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->fr-dep-news-trf==3.8.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->fr-dep-news-trf==3.8.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->fr-dep-news-trf==3.8.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->fr-dep-news-trf==3.8.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->fr-dep-news-trf==3.8.0) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->fr-dep-news-trf==3.8.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->fr-dep-news-trf==3.8.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->fr-dep-news-trf==3.8.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->fr-dep-news-trf==3.8.0) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->fr-dep-news-trf==3.8.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->fr-dep-news-trf==3.8.0) (3.0.3)\n",
            "Downloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading spacy_curated_transformers-0.3.1-py2.py3-none-any.whl (237 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.9/237.9 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading curated_tokenizers-0.0.9-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (734 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m734.0/734.0 kB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading curated_transformers-0.1.1-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: protobuf, curated-tokenizers, curated-transformers, spacy-curated-transformers, fr-dep-news-trf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed curated-tokenizers-0.0.9 curated-transformers-0.1.1 fr-dep-news-trf-3.8.0 protobuf-3.20.3 spacy-curated-transformers-0.3.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_dep_news_trf')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flaubert fait des phrases complexes avec des cascades de subordonnées relatives, c'est difficile de les décortiquer automatiquement et de savoir quel adjectif se rapporte à quel sujet"
      ],
      "metadata": {
        "id": "CmeLDWwTzZ7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Choisis ton modèle : \"fr_dep_news_trf\" (meilleur si dispo) ou \"fr_core_news_md\"\n",
        "nlp = spacy.load(\"fr_dep_news_trf\")\n",
        "\n",
        "def expand_with_conjuncts(token):\n",
        "    \"\"\"Retourne token + ses conjoints (ex. 'Pierre et Paul' -> Pierre, Paul).\"\"\"\n",
        "    results = {token}\n",
        "    head = token.head\n",
        "    # les tokens reliés par dep_ 'conj' qui partagent le même head (ou qui ont token comme head)\n",
        "    for child in head.children:\n",
        "        if child.dep_ == \"conj\":\n",
        "            # cas où target est conj ou où le conjoncteur partage le head\n",
        "            results.add(child)\n",
        "    # cas où token lui-même est marqué 'conj' : rajoute son head si c'est un nom\n",
        "    if token.dep_ == \"conj\" and token.head.pos_ in {\"NOUN\", \"PROPN\", \"PRON\"}:\n",
        "        results.add(token.head)\n",
        "    return list(results)\n",
        "\n",
        "def find_noun_chunk_for(token, doc):\n",
        "    \"\"\"Si le modèle fournit noun_chunks, on essaye de récupérer le syntagme nominal\n",
        "       contenant le token (plus lisible qu'un token isolé).\"\"\"\n",
        "    if not hasattr(doc, \"noun_chunks\"):\n",
        "        return None\n",
        "    for nc in doc.noun_chunks:\n",
        "        if token.i >= nc.start and token.i < nc.end:\n",
        "            return nc\n",
        "    return None\n",
        "\n",
        "def subjects_for_predicative_adj(adj):\n",
        "    \"\"\"\n",
        "    Trouve le(s) sujet(s) pertinent(s) si l'adjectif est prédicatif (attaché à un verbe).\n",
        "    Cherche:\n",
        "      - nsubj enfants du verbe head\n",
        "      - si pas trouvé, cherche un ancêtre nominal\n",
        "      - heuristique: recherche de noms proches à gauche\n",
        "    \"\"\"\n",
        "    verb = adj.head\n",
        "    subs = [c for c in verb.children if c.dep_.startswith(\"nsubj\")]  # 'nsubj', 'nsubj:pass', ...\n",
        "    if subs:\n",
        "        return subs\n",
        "    # si verbe est copule (ex. 'est') mais pas de nsubj trouvé, regarder ancestors\n",
        "    for anc in adj.ancestors:\n",
        "        if anc.pos_ in {\"NOUN\", \"PROPN\", \"PRON\"}:\n",
        "            return [anc]\n",
        "    # heuristique : chercher un nom parmi les tokens précédents dans une petite fenêtre\n",
        "    left_window = [t for t in adj.doc[max(0, adj.i-4):adj.i] if t.pos_ in {\"NOUN\",\"PROPN\",\"PRON\"}]\n",
        "    if left_window:\n",
        "        return [left_window[-1]]\n",
        "    return []\n",
        "\n",
        "def adjective_to_subjects(doc):\n",
        "    \"\"\"\n",
        "    Parcourt les adjectifs et retourne pour chacun:\n",
        "      - l'adjectif,\n",
        "      - sa relation ('attributif' ou 'prédicatif' ou 'inconnu'),\n",
        "      - la/les cibles (sujet/nom) sous forme de spans lisibles.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    for token in doc:\n",
        "        if token.pos_ != \"ADJ\":\n",
        "            continue\n",
        "\n",
        "        # 1) cas attributif direct : adjectif modifie un nom (ex. \"chat blanc\")\n",
        "        if token.dep_ == \"amod\" and token.head.pos_ in {\"NOUN\", \"PROPN\", \"PRON\"}:\n",
        "            targets = expand_with_conjuncts(token.head)\n",
        "            relation = \"attributif (amod)\"\n",
        "            results.append((token, relation, targets))\n",
        "            continue\n",
        "\n",
        "        # 1b) cas adjectif conjoncté à un autre adjectif\n",
        "        if token.dep_ == \"conj\" and token.head.pos_ == \"ADJ\":\n",
        "            # on récupère la cible de l'adj chef si possible\n",
        "            head_adj = token.head\n",
        "            # si le chef modifie un nom directement -> même cible\n",
        "            if head_adj.dep_ == \"amod\" and head_adj.head.pos_ in {\"NOUN\",\"PROPN\",\"PRON\"}:\n",
        "                targets = expand_with_conjuncts(head_adj.head)\n",
        "                relation = \"attributif (via conjonction)\"\n",
        "                results.append((token, relation, targets))\n",
        "                continue\n",
        "            # si le chef est prédicatif (attaché à un verbe)\n",
        "            if head_adj.head.pos_ == \"VERB\":\n",
        "                subs = subjects_for_predicative_adj(head_adj)\n",
        "                if subs:\n",
        "                    relation = \"prédicatif (via conjonction)\"\n",
        "                    results.append((token, relation, subs))\n",
        "                    continue\n",
        "\n",
        "        # 2) cas prédicatif : l'adjectif est lié à un verbe (ex. 'sont heureux')\n",
        "        if token.head.pos_ == \"VERB\" or token.dep_ in {\"acomp\", \"attr\"}:\n",
        "            subs = subjects_for_predicative_adj(token)\n",
        "            if subs:\n",
        "                relation = \"prédicatif (lié au verbe '{}')\".format(token.head.lemma_)\n",
        "                results.append((token, relation, subs))\n",
        "                continue\n",
        "\n",
        "        # 3) heuristique : chercher un ancêtre nominal\n",
        "        noun_anc = None\n",
        "        for anc in token.ancestors:\n",
        "            if anc.pos_ in {\"NOUN\", \"PROPN\", \"PRON\"}:\n",
        "                noun_anc = anc\n",
        "                break\n",
        "        if noun_anc:\n",
        "            targets = expand_with_conjuncts(noun_anc)\n",
        "            relation = \"probablement attributif (via ancêtre nominal)\"\n",
        "            results.append((token, relation, targets))\n",
        "            continue\n",
        "\n",
        "        # 4) dernier recours: chercher un nom proche à gauche\n",
        "        left_window = [t for t in doc[max(0, token.i-4):token.i] if t.pos_ in {\"NOUN\",\"PROPN\",\"PRON\"}]\n",
        "        if left_window:\n",
        "            targets = expand_with_conjuncts(left_window[-1])\n",
        "            relation = \"heuristique (nom proche à gauche)\"\n",
        "            results.append((token, relation, targets))\n",
        "            continue\n",
        "\n",
        "        # 5) rien trouvé\n",
        "        results.append((token, \"inconnu\", []))\n",
        "    return results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    texte = \"Malgré les explications d’Emma, dès le duo récitatif où Gilbert expose à son maître Ashton ses abominables manœuvres, Charles, en voyant le faux anneau de fiançailles qui doit abuser Lucie, crut que c’était un souvenir d’amour envoyé par Edgar.\"\n",
        "    doc = nlp(texte)\n",
        "\n",
        "    infos = adjective_to_subjects(doc)\n",
        "    for adj, relation, targets in infos:\n",
        "        # essayer d'obtenir un syntagme nominal si possible\n",
        "        target_texts = []\n",
        "        for t in targets:\n",
        "            nc = find_noun_chunk_for(t, doc)\n",
        "            if nc:\n",
        "                target_texts.append(f\"'{nc.text}' (tokens {nc.start}-{nc.end-1})\")\n",
        "            else:\n",
        "                target_texts.append(f\"'{t.text}' (index {t.i})\")\n",
        "        if not target_texts:\n",
        "            target_texts = [\"<aucun sujet trouvé>\"]\n",
        "\n",
        "        print(f\"Adjectif: '{adj.text}' (index {adj.i}, dep={adj.dep_})\")\n",
        "        print(f\"  → Rôle détecté : {relation}\")\n",
        "        print(f\"  → Rapporté à : {', '.join(target_texts)}\")\n",
        "        print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQX-QnBuzo5i",
        "outputId": "a46ea6aa-f513-429d-abb9-b3d6cae2d8e1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjectif: 'récitatif' (index 9, dep=amod)\n",
            "  → Rôle détecté : attributif (amod)\n",
            "  → Rapporté à : 'le duo récitatif' (tokens 7-9)\n",
            "\n",
            "Adjectif: 'abominables' (index 18, dep=amod)\n",
            "  → Rôle détecté : attributif (amod)\n",
            "  → Rapporté à : 'ses abominables manœuvres' (tokens 17-19)\n",
            "\n",
            "Adjectif: 'faux' (index 26, dep=amod)\n",
            "  → Rôle détecté : attributif (amod)\n",
            "  → Rapporté à : 'le faux anneau' (tokens 25-27)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Détecter à quel sujet chaque adjectif se rapporte - cas limite\n",
        "\n",
        "texte = \"Puis, s'adressant à Emma, qui portait une robe de soie bleue à quatre falbalas: — Je vous trouve jolie comme un Amour!\"\n",
        "doc = nlp(texte)\n",
        "\n",
        "infos = adjective_to_subjects(doc)\n",
        "for adj, relation, targets in infos:\n",
        "    target_texts = [f\"'{find_noun_chunk_for(t, doc).text}'\" if find_noun_chunk_for(t, doc) else f\"'{t.text}'\"\n",
        "                    for t in targets] or [\"<aucun sujet trouvé>\"]\n",
        "\n",
        "    print(f\"Adjectif: '{adj.text}' (dep={adj.dep_})\")\n",
        "    print(f\"  → Rôle: {relation}\")\n",
        "    print(f\"  → Rapporté à: {', '.join(target_texts)}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "Xw79Sg1a3V_x",
        "outputId": "485a38ea-4b0a-45e5-b28c-3cd58a6c771a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjectif: 'bleue' (dep=amod)\n",
            "  → Rôle: attributif (amod)\n",
            "  → Rapporté à: 'une robe'\n",
            "\n",
            "Adjectif: 'jolie' (dep=xcomp)\n",
            "  → Rôle: prédicatif (lié au verbe 'trouver')\n",
            "  → Rapporté à: 'Je'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import csv\n",
        "from typing import List\n",
        "import spacy\n",
        "\n",
        "# ------------------------------\n",
        "# Your functions (kept as-is, with a small addition for appos)\n",
        "# ------------------------------\n",
        "def expand_with_conjuncts(token):\n",
        "    results = {token}\n",
        "    head = token.head\n",
        "    # les tokens reliés par dep_ 'conj' qui partagent le même head (ou qui ont token comme head)\n",
        "    for child in head.children:\n",
        "        if child.dep_ == \"conj\":\n",
        "            # cas où target est conj ou où le conjoncteur partage le head\n",
        "            results.add(child)\n",
        "    # cas où token lui-même est marqué 'conj' : rajoute son head si c'est un nom\n",
        "    if token.dep_ == \"conj\" and token.head.pos_ in {\"NOUN\", \"PROPN\", \"PRON\"}:\n",
        "        results.add(token.head)\n",
        "    return list(results)\n",
        "\n",
        "\n",
        "def find_noun_chunk_for(token, doc):\n",
        "    \"\"\"Si le modèle fournit noun_chunks, on essaye de récupérer le syntagme nominal\n",
        "       contenant le token (plus lisible qu'un token isolé).\"\"\"\n",
        "    if not hasattr(doc, \"noun_chunks\"):\n",
        "        return None\n",
        "    for nc in doc.noun_chunks:\n",
        "        if token.i >= nc.start and token.i < nc.end:\n",
        "            return nc\n",
        "    return None\n",
        "\n",
        "\n",
        "def subjects_for_predicative_adj(adj):\n",
        "    \"\"\"\n",
        "    Find the relevant subject(s) if the adjective is predicative (attached to a verb).\n",
        "    Cherche:\n",
        "      - nsubj enfants du verbe head\n",
        "      - si pas trouvé, cherche un ancêtre nominal\n",
        "      - heuristique: recherche de noms proches à gauche\n",
        "    \"\"\"\n",
        "    verb = adj.head\n",
        "    subs = [c for c in verb.children if c.dep_.startswith(\"nsubj\")]  # 'nsubj', 'nsubj:pass', ...\n",
        "    if subs:\n",
        "        return subs\n",
        "    # si verbe est copule (ex. 'est') mais pas de nsubj trouvé, regarder ancestors\n",
        "    for anc in adj.ancestors:\n",
        "        if anc.pos_ in {\"NOUN\", \"PROPN\", \"PRON\"}:\n",
        "            return [anc]\n",
        "    # heuristique : chercher un nom parmi les tokens précédents dans une petite fenêtre\n",
        "    left_window = [t for t in adj.doc[max(0, adj.i-4):adj.i] if t.pos_ in {\"NOUN\",\"PROPN\",\"PRON\"}]\n",
        "    if left_window:\n",
        "        return [left_window[-1]]\n",
        "    return []\n",
        "\n",
        "\n",
        "def adjective_to_subjects(doc):\n",
        "    \"\"\"\n",
        "    Parcourt les adjectifs et retourne pour chacun:\n",
        "      - l'adjectif,\n",
        "      - sa relation ('attributif' ou 'prédicatif' ou 'inconnu'),\n",
        "      - la/les cibles (sujet/nom) sous forme de tokens.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    for token in doc:\n",
        "        if token.pos_ != \"ADJ\":\n",
        "            continue\n",
        "\n",
        "        # 1) cas attributif direct : adjectif modifie un nom (ex. \"chat blanc\")\n",
        "        if token.dep_ == \"amod\" and token.head.pos_ in {\"NOUN\", \"PROPN\", \"PRON\"}:\n",
        "            targets = expand_with_conjuncts(token.head)\n",
        "            relation = \"attributif (amod)\"\n",
        "            results.append((token, relation, targets))\n",
        "            continue\n",
        "\n",
        "        # 1b) cas adjectif conjoncté à un autre adjectif\n",
        "        if token.dep_ == \"conj\" and token.head.pos_ == \"ADJ\":\n",
        "            # on récupère la cible de l'adj chef si possible\n",
        "            head_adj = token.head\n",
        "            # si le chef modifie un nom directement -> même cible\n",
        "            if head_adj.dep_ == \"amod\" and head_adj.head.pos_ in {\"NOUN\",\"PROPN\",\"PRON\"}:\n",
        "                targets = expand_with_conjuncts(head_adj.head)\n",
        "                relation = \"attributif (via conjonction)\"\n",
        "                results.append((token, relation, targets))\n",
        "                continue\n",
        "            # si le chef est prédicatif (attaché à un verbe)\n",
        "            if head_adj.head.pos_ == \"VERB\":\n",
        "                subs = subjects_for_predicative_adj(head_adj)\n",
        "                if subs:\n",
        "                    relation = \"prédicatif (via conjonction)\"\n",
        "                    results.append((token, relation, subs))\n",
        "                    continue\n",
        "\n",
        "        # 1c) cas appositif (ex. \"Emma, triste, ...\")\n",
        "        if token.dep_ == \"appos\" and token.head.pos_ in {\"NOUN\", \"PROPN\", \"PRON\"}:\n",
        "            targets = expand_with_conjuncts(token.head)\n",
        "            relation = \"attributif (appos)\"\n",
        "            results.append((token, relation, targets))\n",
        "            continue\n",
        "\n",
        "        # 2) cas prédicatif : l'adjectif est lié à un verbe (ex. 'sont heureux')\n",
        "        if token.head.pos_ == \"VERB\" or token.dep_ in {\"acomp\", \"attr\"}:\n",
        "            subs = subjects_for_predicative_adj(token)\n",
        "            if subs:\n",
        "                relation = \"prédicatif (lié au verbe '{}')\".format(token.head.lemma_)\n",
        "                results.append((token, relation, subs))\n",
        "                continue\n",
        "\n",
        "        # 3) heuristique : chercher un ancêtre nominal\n",
        "        noun_anc = None\n",
        "        for anc in token.ancestors:\n",
        "            if anc.pos_ in {\"NOUN\", \"PROPN\", \"PRON\"}:\n",
        "                noun_anc = anc\n",
        "                break\n",
        "        if noun_anc:\n",
        "            targets = expand_with_conjuncts(noun_anc)\n",
        "            relation = \"probablement attributif (via ancêtre nominal)\"\n",
        "            results.append((token, relation, targets))\n",
        "            continue\n",
        "\n",
        "        # 4) dernier recours: chercher un nom proche à gauche\n",
        "        left_window = [t for t in doc[max(0, token.i-4):token.i] if t.pos_ in {\"NOUN\",\"PROPN\",\"PRON\"}]\n",
        "        if left_window:\n",
        "            targets = expand_with_conjuncts(left_window[-1])\n",
        "            relation = \"heuristique (nom proche à gauche)\"\n",
        "            results.append((token, relation, targets))\n",
        "            continue\n",
        "\n",
        "        # 5) rien trouvé\n",
        "        results.append((token, \"inconnu\", []))\n",
        "    return results\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Helpers to decide if a target refers to Emma (or 'elle')\n",
        "# ------------------------------\n",
        "EMMA_FORMS = {\"emma\"}\n",
        "SHE_FORMS = {\"elle\"}  # we accept 'elle' as referring to Emma per your assumption\n",
        "\n",
        "def is_she_pronoun(token):\n",
        "    # Strictly accept the form \"elle\" (case-insensitive). If needed, loosen with morph features.\n",
        "    if token.pos_ != \"PRON\":\n",
        "        return False\n",
        "    txt = token.text.lower()\n",
        "    if txt in SHE_FORMS or txt.startswith(\"elle-\") or txt.replace(\"’\", \"'\").startswith(\"elle'\"):\n",
        "        return True\n",
        "    # Morphological fallback: feminine, singular, 3rd person\n",
        "    g = token.morph.get(\"Gender\")\n",
        "    n = token.morph.get(\"Number\")\n",
        "    p = token.morph.get(\"Person\")\n",
        "    return (\"Fem\" in g) and (\"Sing\" in n) and (\"3\" in \"\".join(p))\n",
        "\n",
        "def is_token_emma(token):\n",
        "    return token.text.lower() in EMMA_FORMS or token.lemma_.lower() in EMMA_FORMS\n",
        "\n",
        "def in_same_appellation_as_emma(target):\n",
        "    \"\"\"\n",
        "    Heuristic for cases like 'Mademoiselle Emma' where the head might be 'Mademoiselle'\n",
        "    and 'Emma' is attached via 'flat', 'appos', etc. Avoids counting 'mère d’Emma'.\n",
        "    \"\"\"\n",
        "    # Direct name\n",
        "    if is_token_emma(target):\n",
        "        return True\n",
        "\n",
        "    # Check immediate 'name-like' relations\n",
        "    name_like_labels = (\"flat\", \"flat:name\", \"appos\", \"compound\", \"name\")\n",
        "    for child in target.children:\n",
        "        if is_token_emma(child) and (child.dep_ in name_like_labels or child.dep_.startswith(\"flat\")):\n",
        "            return True\n",
        "    if target.dep_ in name_like_labels or target.dep_.startswith(\"flat\"):\n",
        "        if is_token_emma(target.head):\n",
        "            return True\n",
        "\n",
        "    # Quick adjacency heuristic: token immediately followed by 'Emma' (e.g., \"Mademoiselle Emma\")\n",
        "    try:\n",
        "        next_tok = target.nbor(1)\n",
        "        if is_token_emma(next_tok):\n",
        "            return True\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return False\n",
        "\n",
        "def target_refers_to_emma(target, doc):\n",
        "    \"\"\"Return True if the target token is Emma or 'elle', or if the target is in a proper-name span with 'Emma'.\"\"\"\n",
        "    # 'Emma' directly\n",
        "    if is_token_emma(target):\n",
        "        return True\n",
        "\n",
        "    # 'Elle' as subject pronoun (assumed to be Emma)\n",
        "    if is_she_pronoun(target):\n",
        "        return True\n",
        "\n",
        "    # If target is a NOUN/PROPN in the same 'name' span as Emma (e.g., 'Mademoiselle Emma')\n",
        "    if in_same_appellation_as_emma(target):\n",
        "        return True\n",
        "\n",
        "    # If within a noun chunk that has Emma as part of the name (not as genitive 'de Emma')\n",
        "    nc = find_noun_chunk_for(target, doc)\n",
        "    if nc:\n",
        "        root = nc.root\n",
        "        for t in nc:\n",
        "            if is_token_emma(t):\n",
        "                # Accept only if Emma functions as the head or as 'flat/appos' inside the same name\n",
        "                # Reject genitives like 'mère d’Emma'\n",
        "                if t == root or t.dep_.startswith(\"flat\") or t.dep_ in {\"appos\", \"compound\", \"name\"}:\n",
        "                    return True\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# IO helpers\n",
        "# ------------------------------\n",
        "def read_sentences(json_path: str, csv_path: str) -> List[str]:\n",
        "    sentences = []\n",
        "\n",
        "    # Prefer JSON if it exists (the 'sentence' field), otherwise fallback to CSV 3rd column\n",
        "    if os.path.exists(json_path):\n",
        "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "        for item in data:\n",
        "            if isinstance(item, dict) and \"sentence\" in item:\n",
        "                sent = item[\"sentence\"].strip()\n",
        "                if sent:\n",
        "                    sentences.append(sent)\n",
        "\n",
        "    if not sentences and os.path.exists(csv_path):\n",
        "        with open(csv_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            reader = csv.reader(f)\n",
        "            for row in reader:\n",
        "                if len(row) >= 3:\n",
        "                    sent = row[2].strip()\n",
        "                    if sent:\n",
        "                        sentences.append(sent)\n",
        "\n",
        "    # deduplicate while preserving order\n",
        "    seen = set()\n",
        "    unique_sentences = []\n",
        "    for s in sentences:\n",
        "        if s not in seen:\n",
        "            unique_sentences.append(s)\n",
        "            seen.add(s)\n",
        "    return unique_sentences\n",
        "\n",
        "\n",
        "def write_results(rows, out_path: str):\n",
        "    # rows: list of dicts with keys 'sentence', 'adjective'\n",
        "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "    with open(out_path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=[\"sentence\", \"adjective\"])\n",
        "        writer.writeheader()\n",
        "        writer.writerows(rows)\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Main extraction\n",
        "# ------------------------------\n",
        "def extract_emma_adjectives(\n",
        "    json_path=\"/content/ner_bovary/emma.json\",\n",
        "    csv_path=\"/content/ner_bovary/emma.csv\",\n",
        "    out_path=\"/content/ner_bovary/emma_adjectives.csv\",\n",
        "    batch_size=32\n",
        "):\n",
        "    sentences = read_sentences(json_path, csv_path)\n",
        "    if not sentences:\n",
        "        raise FileNotFoundError(\"No sentences found. Check paths to JSON/CSV.\")\n",
        "\n",
        "    # Load spaCy French transformer model (NER not needed here)\n",
        "    nlp = spacy.load(\"fr_dep_news_trf\", disable=[\"ner\"])\n",
        "\n",
        "    results = []\n",
        "    for doc in nlp.pipe(sentences, batch_size=batch_size):\n",
        "        # For each adjective in the doc, find its targets and keep only those that refer to Emma\n",
        "        triples = adjective_to_subjects(doc)\n",
        "        for adj_token, relation, targets in triples:\n",
        "            # Keep if any of the targets is Emma or 'elle'\n",
        "            if any(target_refers_to_emma(t, doc) for t in targets):\n",
        "                results.append({\n",
        "                    \"sentence\": doc.text,\n",
        "                    \"adjective\": adj_token.text\n",
        "                })\n",
        "\n",
        "    write_results(results, out_path)\n",
        "    print(f\"Processed {len(sentences)} passages; found {len(results)} Emma-linked adjectives.\")\n",
        "    print(f\"Saved to {out_path}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    extract_emma_adjectives()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LthFeqh-9vXW",
        "outputId": "1720a6f6-bca4-40e4-86d1-38158fb6cb34"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 428 passages; found 55 Emma-linked adjectives.\n",
            "Saved to /content/ner_bovary/emma_adjectives.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "\n",
        "df = pl.read_csv(\"/content/ner_bovary/emma_adjectives.csv\")\n",
        "col = df.select(df.columns[1])\n",
        "for val in col.to_series():\n",
        "    print(val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTGdSgrE_oPO",
        "outputId": "81038a43-13a1-4e15-dfd3-f365ab0b9113"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pleine\n",
            "bonne\n",
            "toute\n",
            "silencieuse\n",
            "silencieuse\n",
            "disposée\n",
            "difficile\n",
            "capricieuse\n",
            "première\n",
            "première\n",
            "tels\n",
            "faible\n",
            "longue\n",
            "amoureuse\n",
            "toute\n",
            "seule\n",
            "laide\n",
            "joyeuse\n",
            "silencieuse\n",
            "silencieuse\n",
            "silencieuse\n",
            "pareilles\n",
            "pareilles\n",
            "prête\n",
            "courte\n",
            "courte\n",
            "pauvre\n",
            "anxieuse\n",
            "fraîche\n",
            "embarrass\n",
            "toutes\n",
            "docile\n",
            "docile\n",
            "agonisante\n",
            "tous\n",
            "invincible\n",
            "pâle\n",
            "pâle\n",
            "blanche\n",
            "seule\n",
            "ivre\n",
            "ivre\n",
            "telle\n",
            "enflammée\n",
            "avide\n",
            "folle\n",
            "bonne\n",
            "agitée\n",
            "faible\n",
            "faible\n",
            "mignonne\n",
            "seule\n",
            "pâle\n",
            "désespérée\n",
            "petite\n"
          ]
        }
      ]
    }
  ]
}