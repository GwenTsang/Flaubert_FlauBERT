{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfHris-ucKQh",
        "outputId": "688eac75-c002-42de-e587-54de1608e5be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-17 09:14:20--  https://raw.githubusercontent.com/GwenTsang/Flaubert_FlauBERT/main/romans_Flaubert/Bouvard_et_Pecuchet.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 561804 (549K) [text/plain]\n",
            "Saving to: ‘Bouvard_et_Pecuchet.txt.1’\n",
            "\n",
            "Bouvard_et_Pecuchet 100%[===================>] 548.64K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-10-17 09:14:20 (14.1 MB/s) - ‘Bouvard_et_Pecuchet.txt.1’ saved [561804/561804]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/GwenTsang/Flaubert_FlauBERT/main/romans_Flaubert/Bouvard_et_Pecuchet.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Méthode sophistiquée pour l'extraction des chapitres\n",
        "\n",
        "import re\n",
        "from collections import OrderedDict\n",
        "from typing import List, Optional, Dict, Iterable\n",
        "import polars as pl\n",
        "\n",
        "ROMAN_RE = re.compile(\n",
        "    r'^(?=.)M{0,3}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})\\.?$',\n",
        "    re.IGNORECASE\n",
        ")\n",
        "ROMAN_TOKEN_RE = re.compile(r'\\b([IVXLCDM]+)\\b\\.?', re.IGNORECASE)\n",
        "CHAP_RE = re.compile(r'^\\s*(CHAPITRE|CHAP|CHAPTER)\\b\\.?\\s*(.*)$', re.IGNORECASE)\n",
        "\n",
        "\n",
        "def _norm_roman(tok: str) -> str:\n",
        "    return tok.rstrip('.').upper()\n",
        "\n",
        "\n",
        "def scan_headings_polars(path: str, prefer_chapitre: bool = True, allow_roman: bool = True, one_based: bool = True) -> pl.DataFrame:\n",
        "    \"\"\"\n",
        "    Parcourt le fichier ligne par ligne et retourne un pl.DataFrame avec :\n",
        "      - 'element' : chaîne (ex. \"I\" ou \"CHAPITRE I\")\n",
        "      - 'line_indices' : liste d'entiers (indices de lignes, 1-based par défaut)\n",
        "    \"\"\"\n",
        "    with open(path, encoding='utf-8') as f:\n",
        "        lines = f.read().splitlines()\n",
        "\n",
        "    n = len(lines)\n",
        "    used = set()\n",
        "    headings = OrderedDict()\n",
        "\n",
        "    def add(key: str, idxs: List[int]):\n",
        "        if one_based:\n",
        "            idxs = [i + 1 for i in idxs]\n",
        "        headings.setdefault(key, []).extend(idxs)\n",
        "\n",
        "    i = 0\n",
        "    while i < n:\n",
        "        text = lines[i].strip()\n",
        "        if not text:\n",
        "            i += 1\n",
        "            continue\n",
        "\n",
        "        if prefer_chapitre:\n",
        "            m = CHAP_RE.match(text)\n",
        "            if m:\n",
        "                rest = m.group(2).strip()\n",
        "                consumed = [i]\n",
        "                roman = None\n",
        "                if rest:\n",
        "                    t = ROMAN_TOKEN_RE.search(rest)\n",
        "                    if t and ROMAN_RE.fullmatch(t.group(1)):\n",
        "                        roman = _norm_roman(t.group(1))\n",
        "                if not roman:\n",
        "                    j = i + 1\n",
        "                    while j < n and not lines[j].strip():\n",
        "                        j += 1\n",
        "                    if j < n and allow_roman and ROMAN_RE.match(lines[j].strip()):\n",
        "                        roman = _norm_roman(lines[j].strip())\n",
        "                        consumed.append(j)\n",
        "                        used.add(j)\n",
        "                key = f\"CHAPITRE {roman}\" if roman else lines[i].strip()\n",
        "                add(key, consumed)\n",
        "                used.add(i)\n",
        "                i += 1\n",
        "                continue\n",
        "\n",
        "        if allow_roman and i not in used and ROMAN_RE.match(text):\n",
        "            roman = _norm_roman(text)\n",
        "            add(roman, [i])\n",
        "            used.add(i)\n",
        "            i += 1\n",
        "            continue\n",
        "\n",
        "        i += 1\n",
        "\n",
        "    elements = list(headings.keys())\n",
        "    indices = [headings[k] for k in elements]\n",
        "    df = pl.DataFrame({\"element\": elements, \"line_indices\": indices})\n",
        "    return df\n",
        "\n",
        "def _roman_to_int(s: str) -> int:\n",
        "    s = _norm_roman(s)\n",
        "    vals = {'I': 1, 'V': 5, 'X': 10, 'L': 50, 'C': 100, 'D': 500, 'M': 1000}\n",
        "    total = 0\n",
        "    prev = 0\n",
        "    for ch in reversed(s):\n",
        "        v = vals[ch]\n",
        "        if v < prev:\n",
        "            total -= v\n",
        "        else:\n",
        "            total += v\n",
        "            prev = v\n",
        "    return total\n",
        "\n",
        "def _int_to_roman(n: int) -> str:\n",
        "    numerals = [\n",
        "        (1000, 'M'), (900, 'CM'), (500, 'D'), (400, 'CD'),\n",
        "        (100, 'C'), (90, 'XC'), (50, 'L'), (40, 'XL'),\n",
        "        (10, 'X'), (9, 'IX'), (5, 'V'), (4, 'IV'), (1, 'I'),\n",
        "    ]\n",
        "    res = []\n",
        "    for val, sym in numerals:\n",
        "        while n >= val:\n",
        "            res.append(sym)\n",
        "            n -= val\n",
        "    return ''.join(res)\n",
        "\n",
        "def _extract_roman_from_element(element: str) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Returns a normalized Roman numeral token from an 'element' value\n",
        "    like 'CHAPITRE XII' or 'III'. None if no valid Roman token is found.\n",
        "    \"\"\"\n",
        "    s = element.strip()\n",
        "    if ROMAN_RE.fullmatch(s):\n",
        "        return _norm_roman(s)\n",
        "    m = ROMAN_TOKEN_RE.search(s)\n",
        "    if m:\n",
        "        tok = _norm_roman(m.group(1))\n",
        "        if ROMAN_RE.fullmatch(tok):\n",
        "            return tok\n",
        "    return None\n",
        "\n",
        "def _build_candidates_by_roman(df: pl.DataFrame) -> Dict[str, List[int]]:\n",
        "    by_roman: Dict[str, List[int]] = {}\n",
        "    for element, idxs in df.iter_rows():\n",
        "        roman = _extract_roman_from_element(element)\n",
        "        if not roman:\n",
        "            continue\n",
        "        if not isinstance(idxs, list) or not idxs:\n",
        "            continue\n",
        "        by_roman.setdefault(roman, []).extend(int(x) for x in idxs)\n",
        "    for r, lst in by_roman.items():\n",
        "        by_roman[r] = sorted(set(lst))\n",
        "    return by_roman\n",
        "\n",
        "_LETTER_TOKEN_RE = re.compile(r\"[^\\W\\d_]+\", flags=re.UNICODE)\n",
        "\n",
        "def _has_all_caps_word(line: str, min_len: int = 2) -> bool:\n",
        "    \"\"\"\n",
        "    True if the line contains a word of only letters, length >= min_len, and all letters uppercase.\n",
        "    Examples that count: 'PREFACE', 'TABLE', 'III' (roman in caps), 'PRÉFACE'\n",
        "    Examples that do NOT count: 'John', 'I' (single letter), 'Title-Case' (split as 'Title', 'Case' -> both not isupper)\n",
        "    \"\"\"\n",
        "    for tok in _LETTER_TOKEN_RE.findall(line):\n",
        "        if len(tok) >= min_len and tok.isupper():\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def _invalidates_first_I(lines: List[str], i_one_based: int, window: Iterable[int] = (1, 2, 3), require_full_window: bool = True) -> bool:\n",
        "    \"\"\"\n",
        "    Returns True if candidate index i for chapter I should be rejected:\n",
        "      - any of lines i+1, i+2, i+3 has an ALL-CAPS word (length >= 2), OR\n",
        "      - require_full_window=True and any of those lines do not exist.\n",
        "    \"\"\"\n",
        "    n = len(lines)\n",
        "    i0 = i_one_based - 1\n",
        "    if require_full_window and any(i0 + off >= n for off in window):\n",
        "        return True\n",
        "    for off in window:\n",
        "        j = i0 + off\n",
        "        if 0 <= j < n:\n",
        "            if _has_all_caps_word(lines[j].lstrip(), min_len=2):\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "def select_monotonic_chapters(\n",
        "    df: pl.DataFrame,\n",
        "    target_max: int = 26,\n",
        "    require_complete: bool = False,\n",
        "    keep_candidates: bool = False,\n",
        "    min_lines: int = 8,\n",
        "    enforce_last_min: bool = False,\n",
        "    total_lines: Optional[int] = None,\n",
        "    lines: Optional[List[str]] = None,\n",
        "    require_full_window_for_I: bool = True,\n",
        ") -> pl.DataFrame:\n",
        "    \"\"\"\n",
        "    Pick one line index per chapter (I..target_max) so that:\n",
        "      - indices strictly increase\n",
        "      - each chapter start is at least `min_lines` after the previous start\n",
        "      - special rule for chapter I:\n",
        "          reject a candidate i if any of i+1, i+2, i+3 has an ALL-CAPS word (len >= 2),\n",
        "          or (optionally) if those lookahead lines are missing.\n",
        "\n",
        "    If require_complete=True, raises if a chapter cannot be placed; otherwise it is skipped.\n",
        "    Set keep_candidates=True to include the deduped candidates per chapter.\n",
        "\n",
        "    Pass `lines` as the raw file lines (list of str) to enable the chapter I rule.\n",
        "    \"\"\"\n",
        "    by_roman = _build_candidates_by_roman(df)\n",
        "    order = [_int_to_roman(i) for i in range(1, target_max + 1)]\n",
        "    out_rows = []\n",
        "    prev = 0\n",
        "\n",
        "    for r in order:\n",
        "        candidates = by_roman.get(r, [])\n",
        "        threshold = 1 if prev == 0 else prev + min_lines\n",
        "\n",
        "        chosen = None\n",
        "        if r == \"I\" and lines is not None:\n",
        "            for x in candidates:\n",
        "                if x >= threshold and not _invalidates_first_I(lines, x, require_full_window=require_full_window_for_I):\n",
        "                    chosen = x\n",
        "                    break\n",
        "        else:\n",
        "            chosen = next((x for x in candidates if x >= threshold), None)\n",
        "\n",
        "        if chosen is None:\n",
        "            if require_complete:\n",
        "                rule = \" with I-rule\" if (r == \"I\" and lines is not None) else \"\"\n",
        "                raise ValueError(f\"No feasible index for chapter {r}{rule}. Threshold={threshold}. Candidates={candidates}\")\n",
        "            continue\n",
        "\n",
        "        row = {\"element\": r, \"line\": chosen}\n",
        "        if keep_candidates:\n",
        "            row[\"candidates\"] = candidates\n",
        "        out_rows.append(row)\n",
        "        prev = chosen\n",
        "\n",
        "    if enforce_last_min and total_lines is not None:\n",
        "        while out_rows:\n",
        "            last_line = out_rows[-1][\"line\"]\n",
        "            if (total_lines - last_line + 1) >= min_lines:\n",
        "                break\n",
        "            if require_complete:\n",
        "                raise ValueError(\n",
        "                    f\"Last chapter {out_rows[-1]['element']} at line {last_line} \"\n",
        "                    f\"has fewer than {min_lines} lines until EOF ({total_lines}).\"\n",
        "                )\n",
        "            out_rows.pop()\n",
        "\n",
        "    return pl.from_dicts(out_rows)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "mR69g8ssc13F"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Application des fonctions et constitution d'un dictionnaire\n",
        "\n",
        "from glob import glob\n",
        "import os\n",
        "import polars as pl\n",
        "\n",
        "\n",
        "folder = \"/content\"\n",
        "\n",
        "\n",
        "def load_chapters(path, start_lines):\n",
        "    \"\"\"Extract chapters from a text file based on start line indices.\"\"\"\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    n_lines = len(lines)\n",
        "    # Verification and conversion to 0-based indices\n",
        "    starts = []\n",
        "    for roman, lineno in start_lines.items():\n",
        "        if lineno < 1 or lineno > n_lines:\n",
        "            raise ValueError(f\"Invalid start index for chapter {roman}: line {lineno} (file has {n_lines} lines)\")\n",
        "        starts.append((roman, lineno - 1))\n",
        "\n",
        "    starts.sort(key=lambda x: x[1])\n",
        "\n",
        "    # Build the dictionary { 'I': 'text of chapter I', ... }\n",
        "    chapters = {}\n",
        "    for i, (roman, start_idx) in enumerate(starts):\n",
        "        end_idx = starts[i+1][1] - 1 if i < len(starts) - 1 else n_lines - 1\n",
        "        chap_text = ''.join(lines[start_idx:end_idx+1]).rstrip('\\n')\n",
        "        chapters[roman] = chap_text\n",
        "\n",
        "    return chapters\n",
        "\n",
        "\n",
        "def build_starts_index(folder: str, target_max: int = 26, min_lines: int = 8):\n",
        "    \"\"\"Build an index of chapter starts from all text files in a folder.\"\"\"\n",
        "    start_rows = []\n",
        "    starts_map = {}\n",
        "\n",
        "    for path in sorted(glob(f\"{folder}/*.txt\")):\n",
        "        with open(path, encoding=\"utf-8\") as f:\n",
        "            lines = f.read().splitlines()\n",
        "\n",
        "        df_scan = scan_headings_polars(path)\n",
        "        selected = select_monotonic_chapters(\n",
        "            df_scan,\n",
        "            target_max=target_max,\n",
        "            min_lines=min_lines,\n",
        "            lines=lines,\n",
        "        )\n",
        "\n",
        "        if selected.height == 0:\n",
        "            continue\n",
        "\n",
        "        fname = os.path.basename(path)\n",
        "        for roman, line in selected.select([\"element\", \"line\"]).iter_rows():\n",
        "            start_rows.append({\n",
        "                \"file\": fname,\n",
        "                \"path\": path,\n",
        "                \"chapter\": roman,\n",
        "                \"start_line\": int(line),\n",
        "            })\n",
        "            starts_map.setdefault(path, {})[roman] = int(line)\n",
        "\n",
        "    starts_df = (\n",
        "        pl.from_dicts(start_rows).sort([\"file\", \"start_line\"])\n",
        "        if start_rows else\n",
        "        pl.DataFrame({\"file\": [], \"path\": [], \"chapter\": [], \"start_line\": []})\n",
        "    )\n",
        "    return starts_df, starts_map\n",
        "\n",
        "\n",
        "# Build chapter start index\n",
        "starts_df, starts_map = build_starts_index(folder, target_max=26, min_lines=8)\n",
        "print(\"Chapter start index DataFrame:\")\n",
        "print(starts_df)\n",
        "\n",
        "# Load chapters for each book\n",
        "book_chapters = {}\n",
        "for path, start_lines in starts_map.items():\n",
        "    fname = os.path.basename(path)\n",
        "    chapters = load_chapters(path, start_lines)\n",
        "    book_chapters[fname] = chapters"
      ],
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rtw3dmufeEe2",
        "outputId": "72de1571-68f4-40ab-a66a-a7ae9500e12a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chapter start index DataFrame:\n",
            "shape: (10, 4)\n",
            "┌─────────────────────────┬─────────────────────────────────┬─────────┬────────────┐\n",
            "│ file                    ┆ path                            ┆ chapter ┆ start_line │\n",
            "│ ---                     ┆ ---                             ┆ ---     ┆ ---        │\n",
            "│ str                     ┆ str                             ┆ str     ┆ i64        │\n",
            "╞═════════════════════════╪═════════════════════════════════╪═════════╪════════════╡\n",
            "│ Bouvard_et_Pecuchet.txt ┆ /content/Bouvard_et_Pecuchet.t… ┆ I       ┆ 19         │\n",
            "│ Bouvard_et_Pecuchet.txt ┆ /content/Bouvard_et_Pecuchet.t… ┆ II      ┆ 331        │\n",
            "│ Bouvard_et_Pecuchet.txt ┆ /content/Bouvard_et_Pecuchet.t… ┆ III     ┆ 939        │\n",
            "│ Bouvard_et_Pecuchet.txt ┆ /content/Bouvard_et_Pecuchet.t… ┆ IV      ┆ 1865       │\n",
            "│ Bouvard_et_Pecuchet.txt ┆ /content/Bouvard_et_Pecuchet.t… ┆ V       ┆ 2643       │\n",
            "│ Bouvard_et_Pecuchet.txt ┆ /content/Bouvard_et_Pecuchet.t… ┆ VI      ┆ 3211       │\n",
            "│ Bouvard_et_Pecuchet.txt ┆ /content/Bouvard_et_Pecuchet.t… ┆ VII     ┆ 4055       │\n",
            "│ Bouvard_et_Pecuchet.txt ┆ /content/Bouvard_et_Pecuchet.t… ┆ VIII    ┆ 4337       │\n",
            "│ Bouvard_et_Pecuchet.txt ┆ /content/Bouvard_et_Pecuchet.t… ┆ IX      ┆ 5433       │\n",
            "│ Bouvard_et_Pecuchet.txt ┆ /content/Bouvard_et_Pecuchet.t… ┆ X       ┆ 6357       │\n",
            "└─────────────────────────┴─────────────────────────────────┴─────────┴────────────┘\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define input and output file paths\n",
        "input_file = \"/content/Bouvard_et_Pecuchet.txt\"\n",
        "output_file = \"/content/Bouvard_et_Pecuchet_chapitre_1.txt\"\n",
        "\n",
        "# Define the line range to retain\n",
        "start_line = 22\n",
        "end_line = 330\n",
        "\n",
        "# Open the input file and extract the required lines\n",
        "with open(input_file, \"r\", encoding=\"utf-8\") as infile:\n",
        "    lines = infile.readlines()\n",
        "\n",
        "# Select only lines 19 to 331 (Python uses 0-based indexing)\n",
        "selected_lines = lines[start_line - 1:end_line]\n",
        "\n",
        "# Write the selected lines to a new file\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
        "    outfile.writelines(selected_lines)\n",
        "\n",
        "print(f\"Lines {start_line} to {end_line} have been saved to {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOtcNbbpecZc",
        "outputId": "8e834c46-b275-4b57-d642-74de3cb76eb8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lines 22 to 330 have been saved to /content/Bouvard_et_Pecuchet_chapitre_1.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "file_path = \"/content/Bouvard_et_Pecuchet_chapitre_1.txt\"\n",
        "\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "# This pattern splits at '.', '?', or '!' followed by a space or end of line\n",
        "sentences = re.split(r'[.!?]+(?:\\s|$)', text)\n",
        "\n",
        "sentences = [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "num_sentences = len(sentences)\n",
        "\n",
        "print(f\"Number of sentences in the file: {num_sentences}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eM1aaT6neydc",
        "outputId": "c0f5c479-84aa-455a-89d4-0b583276e52b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentences in the file: 389\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sacremoses torch -q"
      ],
      "metadata": {
        "id": "V_QXlHFqcjQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"flaubert/flaubert_base_uncased\")\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"flaubert/flaubert_base_uncased\")"
      ],
      "metadata": {
        "id": "i0eiuDrZckil"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}